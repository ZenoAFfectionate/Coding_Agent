"""RLè®­ç»ƒå™¨å°è£…

æœ¬æ¨¡å—å°è£…äº†TRLçš„å„ç§è®­ç»ƒå™¨ï¼Œæä¾›ç»Ÿä¸€çš„æ¥å£ã€‚
"""

from typing import Optional, Callable, Dict, Any
from pathlib import Path

from .utils import TrainingConfig, check_trl_installation, get_installation_guide

try:
    from transformers import TrainerCallback

    class DetailedLoggingCallback(TrainerCallback):
        """è¯¦ç»†æ—¥å¿—å›è°ƒ

        åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è¾“å‡ºæ›´è¯¦ç»†çš„æ—¥å¿—ä¿¡æ¯,åŒ…æ‹¬:
        - Epoch/Stepè¿›åº¦
        - Loss
        - Learning Rate
        - Reward (GRPO)
        - KLæ•£åº¦ (GRPO)
        """

        def __init__(self, total_steps: int = None, num_epochs: int = None):
            """
            åˆå§‹åŒ–å›è°ƒ

            Args:
                total_steps: æ€»æ­¥æ•°
                num_epochs: æ€»è½®æ•°
            """
            self.total_steps = total_steps
            self.num_epochs = num_epochs
            self.current_epoch = 0

        def on_log(self, args, state, control, logs=None, **kwargs):
            """æ—¥å¿—å›è°ƒ"""
            if logs is None:
                return

            # è®¡ç®—å½“å‰epoch
            if state.epoch is not None:
                self.current_epoch = int(state.epoch)

            # æ„å»ºæ—¥å¿—æ¶ˆæ¯
            log_parts = []

            # Epochå’ŒStepä¿¡æ¯
            if self.num_epochs:
                log_parts.append(f"Epoch {self.current_epoch + 1}/{self.num_epochs}")

            if state.global_step and self.total_steps:
                log_parts.append(f"Step {state.global_step}/{self.total_steps}")
            elif state.global_step:
                log_parts.append(f"Step {state.global_step}")

            # Loss
            if "loss" in logs:
                log_parts.append(f"Loss: {logs['loss']:.4f}")

            # Learning Rate
            if "learning_rate" in logs:
                log_parts.append(f"LR: {logs['learning_rate']:.2e}")

            # GRPOç‰¹å®šæŒ‡æ ‡
            if "rewards/mean" in logs:
                log_parts.append(f"Reward: {logs['rewards/mean']:.4f}")

            if "objective/kl" in logs:
                log_parts.append(f"KL: {logs['objective/kl']:.4f}")

            # è¾“å‡ºæ—¥å¿—
            if log_parts:
                print(" | ".join(log_parts))

        def on_epoch_end(self, args, state, control, **kwargs):
            """Epochç»“æŸå›è°ƒ"""
            print(f"{'='*80}")
            print(f"âœ… Epoch {self.current_epoch + 1} å®Œæˆ")
            print(f"{'='*80}\n")

except ImportError:
    # å¦‚æœtransformersæœªå®‰è£…,åˆ›å»ºä¸€ä¸ªç©ºçš„å›è°ƒç±»
    class DetailedLoggingCallback:
        def __init__(self, *args, **kwargs):
            pass


class BaseTrainerWrapper:
    """è®­ç»ƒå™¨åŸºç±»"""
    
    def __init__(self, config: Optional[TrainingConfig] = None):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨
        
        Args:
            config: è®­ç»ƒé…ç½®
        """
        # æ£€æŸ¥TRLæ˜¯å¦å®‰è£…
        if not check_trl_installation():
            raise ImportError(get_installation_guide())
        
        self.config = config or TrainingConfig()
        self.trainer = None
        self.model = None
        self.tokenizer = None
    
    def setup_model(self):
        """è®¾ç½®æ¨¡å‹å’Œtokenizer"""
        raise NotImplementedError
    
    def train(self):
        """å¼€å§‹è®­ç»ƒ"""
        raise NotImplementedError
    
    def save_model(self, output_dir: Optional[str] = None):
        """
        ä¿å­˜æ¨¡å‹
        
        Args:
            output_dir: è¾“å‡ºç›®å½•
        """
        save_dir = output_dir or self.config.output_dir
        if self.trainer:
            self.trainer.save_model(save_dir)
            print(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ°: {save_dir}")
        else:
            print("âŒ è®­ç»ƒå™¨æœªåˆå§‹åŒ–ï¼Œæ— æ³•ä¿å­˜æ¨¡å‹")


class SFTTrainerWrapper(BaseTrainerWrapper):
    """SFT (Supervised Fine-Tuning) è®­ç»ƒå™¨å°è£…
    
    ç”¨äºç›‘ç£å¾®è°ƒï¼Œè®©æ¨¡å‹å­¦ä¼šéµå¾ªæŒ‡ä»¤å’ŒåŸºæœ¬çš„æ¨ç†æ ¼å¼ã€‚
    """
    
    def __init__(
        self,
        config: Optional[TrainingConfig] = None,
        dataset = None
    ):
        """
        åˆå§‹åŒ–SFTè®­ç»ƒå™¨
        
        Args:
            config: è®­ç»ƒé…ç½®
            dataset: è®­ç»ƒæ•°æ®é›†
        """
        super().__init__(config)
        self.dataset = dataset
    
    def setup_model(self):
        """è®¾ç½®æ¨¡å‹å’Œtokenizer"""
        from transformers import AutoModelForCausalLM, AutoTokenizer
        
        print(f"ğŸ“¦ åŠ è½½æ¨¡å‹: {self.config.model_name}")
        
        # åŠ è½½tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.config.model_name,
            trust_remote_code=True
        )
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # åŠ è½½æ¨¡å‹
        self.model = AutoModelForCausalLM.from_pretrained(
            self.config.model_name,
            trust_remote_code=True,
            device_map="auto" if self.config.use_fp16 or self.config.use_bf16 else None
        )
        
        print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    
    def train(self):
        """å¼€å§‹SFTè®­ç»ƒ"""
        from trl import SFTConfig, SFTTrainer
        
        if self.model is None:
            self.setup_model()
        
        if self.dataset is None:
            raise ValueError("æ•°æ®é›†æœªè®¾ç½®ï¼Œè¯·æä¾›è®­ç»ƒæ•°æ®é›†")
        
        # é…ç½®è®­ç»ƒå‚æ•°
        # ç¡®å®šreport_toå‚æ•°
        report_to = []
        if self.config.use_wandb:
            report_to.append("wandb")
        if self.config.use_tensorboard:
            report_to.append("tensorboard")
        if not report_to:
            report_to = ["none"]

        training_args = SFTConfig(
            output_dir=self.config.output_dir,
            num_train_epochs=self.config.num_train_epochs,
            per_device_train_batch_size=self.config.per_device_train_batch_size,
            gradient_accumulation_steps=self.config.gradient_accumulation_steps,
            learning_rate=self.config.learning_rate,
            warmup_steps=self.config.warmup_steps,
            logging_steps=self.config.logging_steps,
            save_steps=self.config.save_steps,
            fp16=self.config.use_fp16,
            bf16=self.config.use_bf16,
            gradient_checkpointing=self.config.gradient_checkpointing,
            max_length=self.config.max_length,  # ä¿®æ­£å‚æ•°å
            report_to=report_to,
        )
        
        # è®¡ç®—æ€»æ­¥æ•°
        total_steps = (
            len(self.dataset) //
            (self.config.per_device_train_batch_size * self.config.gradient_accumulation_steps)
        ) * self.config.num_train_epochs

        # åˆ›å»ºè¯¦ç»†æ—¥å¿—å›è°ƒ
        logging_callback = DetailedLoggingCallback(
            total_steps=total_steps,
            num_epochs=self.config.num_train_epochs
        )

        # åˆ›å»ºè®­ç»ƒå™¨
        self.trainer = SFTTrainer(
            model=self.model,
            args=training_args,
            train_dataset=self.dataset,
            processing_class=self.tokenizer,  # æ–°ç‰ˆTRLä½¿ç”¨processing_class
            callbacks=[logging_callback],  # æ·»åŠ å›è°ƒ
        )

        print("\nğŸš€ å¼€å§‹SFTè®­ç»ƒ...")
        print(f"{'='*80}\n")
        self.trainer.train()
        print(f"\n{'='*80}")
        print("âœ… SFTè®­ç»ƒå®Œæˆ")
        
        return self.trainer


class GRPOTrainerWrapper(BaseTrainerWrapper):
    """GRPO (Group Relative Policy Optimization) è®­ç»ƒå™¨å°è£…
    
    ç”¨äºå¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼Œä¼˜åŒ–æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚
    GRPOç›¸æ¯”PPOæ›´ç®€å•ï¼Œä¸éœ€è¦Value Modelã€‚
    """
    
    def __init__(
        self,
        config: Optional[TrainingConfig] = None,
        dataset = None,
        reward_fn: Optional[Callable] = None
    ):
        """
        åˆå§‹åŒ–GRPOè®­ç»ƒå™¨
        
        Args:
            config: è®­ç»ƒé…ç½®
            dataset: è®­ç»ƒæ•°æ®é›†
            reward_fn: å¥–åŠ±å‡½æ•°
        """
        super().__init__(config)
        self.dataset = dataset
        self.reward_fn = reward_fn
    
    def setup_model(self):
        """è®¾ç½®æ¨¡å‹å’Œtokenizer"""
        from transformers import AutoModelForCausalLM, AutoTokenizer
        
        print(f"ğŸ“¦ åŠ è½½æ¨¡å‹: {self.config.model_name}")
        
        # åŠ è½½tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.config.model_name,
            trust_remote_code=True
        )
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # åŠ è½½æ¨¡å‹
        self.model = AutoModelForCausalLM.from_pretrained(
            self.config.model_name,
            trust_remote_code=True,
            device_map="auto" if self.config.use_fp16 or self.config.use_bf16 else None
        )
        
        print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    
    def train(self):
        """å¼€å§‹GRPOè®­ç»ƒ"""
        from trl import GRPOConfig, GRPOTrainer
        
        if self.model is None:
            self.setup_model()
        
        if self.dataset is None:
            raise ValueError("æ•°æ®é›†æœªè®¾ç½®ï¼Œè¯·æä¾›è®­ç»ƒæ•°æ®é›†")
        
        if self.reward_fn is None:
            raise ValueError("å¥–åŠ±å‡½æ•°æœªè®¾ç½®ï¼Œè¯·æä¾›reward_fn")
        
        # ç¡®å®šreport_toå‚æ•°
        report_to = []
        if self.config.use_wandb:
            report_to.append("wandb")
        if self.config.use_tensorboard:
            report_to.append("tensorboard")
        if not report_to:
            report_to = ["none"]

        # é…ç½®è®­ç»ƒå‚æ•°
        training_args = GRPOConfig(
            output_dir=self.config.output_dir,
            num_train_epochs=self.config.num_train_epochs,
            per_device_train_batch_size=self.config.per_device_train_batch_size,
            gradient_accumulation_steps=self.config.gradient_accumulation_steps,
            learning_rate=self.config.learning_rate,
            warmup_steps=self.config.warmup_steps,
            logging_steps=self.config.logging_steps,
            save_steps=self.config.save_steps,
            fp16=self.config.use_fp16,
            bf16=self.config.use_bf16,
            report_to=report_to,
            remove_unused_columns=False,  # ä¿ç•™æ‰€æœ‰åˆ—,åŒ…æ‹¬ground_truthç­‰
        )
        
        # è®¡ç®—æ€»æ­¥æ•°
        total_steps = (
            len(self.dataset) //
            (self.config.per_device_train_batch_size * self.config.gradient_accumulation_steps)
        ) * self.config.num_train_epochs

        # åˆ›å»ºè¯¦ç»†æ—¥å¿—å›è°ƒ
        logging_callback = DetailedLoggingCallback(
            total_steps=total_steps,
            num_epochs=self.config.num_train_epochs
        )

        # åˆ›å»ºè®­ç»ƒå™¨
        self.trainer = GRPOTrainer(
            model=self.model,
            args=training_args,
            train_dataset=self.dataset,
            reward_funcs=self.reward_fn,
            processing_class=self.tokenizer,
            callbacks=[logging_callback],  # æ·»åŠ å›è°ƒ
        )

        print("\nğŸš€ å¼€å§‹GRPOè®­ç»ƒ...")
        print(f"{'='*80}\n")
        self.trainer.train()
        print(f"\n{'='*80}")
        print("âœ… GRPOè®­ç»ƒå®Œæˆ")
        
        return self.trainer


class PPOTrainerWrapper(BaseTrainerWrapper):
    """PPO (Proximal Policy Optimization) è®­ç»ƒå™¨å°è£…

    ç”¨äºå¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼Œæ˜¯ç»å…¸çš„RLç®—æ³•ã€‚
    ç›¸æ¯”GRPOï¼ŒPPOéœ€è¦é¢å¤–çš„Value Modelå’ŒReward Model (nn.Module)ï¼Œä½†å¯èƒ½è·å¾—æ›´å¥½çš„æ€§èƒ½ã€‚

    æ³¨æ„ï¼šTRL >= 0.24 ä¸­ PPOTrainer å·²è¢«æ ‡è®°ä¸ºå®éªŒæ€§åŠŸèƒ½ï¼ˆcandidate for removalï¼‰ï¼Œ
    å®˜æ–¹æ¨èä½¿ç”¨ GRPOTrainer æˆ– DPOTrainerã€‚å¦‚æœä¸éœ€è¦ Value Model çš„ä¼˜åŠ¿ï¼Œ
    å»ºè®®ä¼˜å…ˆä½¿ç”¨ GRPOTrainerWrapperã€‚

    å…³é”®åŒºåˆ«ï¼ˆPPO vs GRPOï¼‰ï¼š
    - PPO éœ€è¦ reward_model (nn.Module, é€šå¸¸æ˜¯ AutoModelForSequenceClassification)
    - PPO éœ€è¦ value_model (nn.Module, é€šå¸¸ä¹Ÿæ˜¯ AutoModelForSequenceClassification)
    - PPO çš„æ•°æ®é›†æ ¼å¼ä¸ºå·² tokenize çš„ input_idsï¼ˆè€Œé GRPO çš„ prompt å­—ç¬¦ä¸²ï¼‰
    - GRPO ä½¿ç”¨ reward function (Callable)ï¼Œæ›´çµæ´»
    """

    def __init__(
        self,
        config: Optional[TrainingConfig] = None,
        dataset=None,
        reward_model=None,
        reward_model_name: Optional[str] = None,
        value_model=None,
        value_model_name: Optional[str] = None,
        ref_model=None,
        peft_config=None,
    ):
        """
        åˆå§‹åŒ–PPOè®­ç»ƒå™¨

        Args:
            config: è®­ç»ƒé…ç½®
            dataset: è®­ç»ƒæ•°æ®é›†ï¼ˆéœ€åŒ…å« "input_ids" åˆ—ï¼Œæˆ–åŒ…å« "prompt" åˆ—ä¼šè‡ªåŠ¨ tokenizeï¼‰
            reward_model: å¥–åŠ±æ¨¡å‹ (nn.Module, AutoModelForSequenceClassification)ã€‚
                          å¦‚æœæœªæä¾›ï¼Œå°†ä½¿ç”¨ reward_model_name è‡ªåŠ¨åŠ è½½ã€‚
            reward_model_name: å¥–åŠ±æ¨¡å‹åç§°/è·¯å¾„ï¼Œå½“ reward_model æœªæä¾›æ—¶ä½¿ç”¨ã€‚
                               é»˜è®¤ä½¿ç”¨ä¸ç­–ç•¥æ¨¡å‹ç›¸åŒçš„æ¨¡å‹åç§°ã€‚
            value_model: ä»·å€¼æ¨¡å‹ (nn.Module, AutoModelForSequenceClassification)ã€‚
                         å¦‚æœæœªæä¾›ï¼Œå°†ä½¿ç”¨ value_model_name è‡ªåŠ¨åŠ è½½ã€‚
            value_model_name: ä»·å€¼æ¨¡å‹åç§°/è·¯å¾„ï¼Œå½“ value_model æœªæä¾›æ—¶ä½¿ç”¨ã€‚
                              é»˜è®¤ä½¿ç”¨ä¸ç­–ç•¥æ¨¡å‹ç›¸åŒçš„æ¨¡å‹åç§°ã€‚
            ref_model: å‚è€ƒæ¨¡å‹ (nn.Module)ã€‚å¦‚æœæœªæä¾›ä¸”ä¸ä½¿ç”¨ PEFTï¼Œ
                       å°†è‡ªåŠ¨ä»ç­–ç•¥æ¨¡å‹åˆ›å»ºä¸€ä»½å‰¯æœ¬ã€‚è®¾ä¸º None + peft_config
                       æ—¶ä½¿ç”¨ PEFT adapter ä½œä¸ºéšå¼å‚è€ƒã€‚
            peft_config: PEFTé…ç½®ï¼ˆå¦‚LoraConfigï¼‰ï¼Œç”¨äºå‚æ•°é«˜æ•ˆå¾®è°ƒã€‚
                         ä½¿ç”¨PEFTæ—¶æ— éœ€é¢å¤–çš„ref_modelã€‚
        """
        super().__init__(config)
        self.dataset = dataset
        self.reward_model = reward_model
        self.reward_model_name = reward_model_name
        self.value_model = value_model
        self.value_model_name = value_model_name
        self.ref_model = ref_model
        self.peft_config = peft_config

    def setup_model(self):
        """è®¾ç½®ç­–ç•¥æ¨¡å‹ã€å¥–åŠ±æ¨¡å‹ã€ä»·å€¼æ¨¡å‹å’Œ tokenizer"""
        from transformers import (
            AutoModelForCausalLM,
            AutoModelForSequenceClassification,
            AutoTokenizer,
        )

        print(f"ğŸ“¦ åŠ è½½ç­–ç•¥æ¨¡å‹: {self.config.model_name}")

        # åŠ è½½tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.config.model_name,
            trust_remote_code=True,
        )
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token

        # åŠ è½½ç­–ç•¥æ¨¡å‹ (CausalLM)
        self.model = AutoModelForCausalLM.from_pretrained(
            self.config.model_name,
            trust_remote_code=True,
        )
        print("âœ… ç­–ç•¥æ¨¡å‹åŠ è½½å®Œæˆ")

        # åŠ è½½å¥–åŠ±æ¨¡å‹ (SequenceClassification)
        # PPOTrainer é€šè¿‡ model.score() è·å–æ ‡é‡å¥–åŠ±ï¼Œéœ€è¦ num_labels=1
        if self.reward_model is None:
            rm_name = self.reward_model_name or self.config.model_name
            print(f"ğŸ“¦ åŠ è½½å¥–åŠ±æ¨¡å‹: {rm_name}")
            self.reward_model = AutoModelForSequenceClassification.from_pretrained(
                rm_name,
                trust_remote_code=True,
                num_labels=1,
                ignore_mismatched_sizes=True,
            )
            print("âœ… å¥–åŠ±æ¨¡å‹åŠ è½½å®Œæˆ")

        # åŠ è½½ä»·å€¼æ¨¡å‹ (SequenceClassification)
        # PPOTrainer ä½¿ç”¨ value_model.score() è¾“å‡ºé€ token çš„æ ‡é‡ä»·å€¼ä¼°è®¡
        if self.value_model is None:
            vm_name = self.value_model_name or self.config.model_name
            print(f"ğŸ“¦ åŠ è½½ä»·å€¼æ¨¡å‹: {vm_name}")
            self.value_model = AutoModelForSequenceClassification.from_pretrained(
                vm_name,
                trust_remote_code=True,
                num_labels=1,
                ignore_mismatched_sizes=True,
            )
            print("âœ… ä»·å€¼æ¨¡å‹åŠ è½½å®Œæˆ")

        # å‚è€ƒæ¨¡å‹å¤„ç†ï¼šä½¿ç”¨ PEFT æ—¶æ— éœ€å•ç‹¬çš„ ref_model
        if self.ref_model is None and self.peft_config is None:
            print("ğŸ“¦ åˆ›å»ºå‚è€ƒæ¨¡å‹ï¼ˆç­–ç•¥æ¨¡å‹å‰¯æœ¬ï¼‰")
            import copy
            self.ref_model = copy.deepcopy(self.model)
            print("âœ… å‚è€ƒæ¨¡å‹åˆ›å»ºå®Œæˆ")

    def prepare_dataset(self, dataset=None):
        """å°†æ•°æ®é›†è½¬æ¢ä¸º PPOTrainer æ‰€éœ€çš„ tokenized æ ¼å¼

        PPOTrainer æœŸæœ›æ•°æ®é›†åŒ…å« 'input_ids' åˆ—ï¼ˆtokenized prompt tensorsï¼‰ã€‚
        å¦‚æœæ•°æ®é›†åŒ…å« 'prompt' åˆ—ï¼ˆå­—ç¬¦ä¸²ï¼‰ï¼Œå°†è‡ªåŠ¨è¿›è¡Œ tokenizationã€‚

        Args:
            dataset: å¯é€‰çš„æ•°æ®é›†ï¼Œæœªæä¾›æ—¶ä½¿ç”¨ self.dataset

        Returns:
            tokenized åçš„ Dataset
        """
        ds = dataset or self.dataset
        if ds is None:
            raise ValueError("æ•°æ®é›†æœªè®¾ç½®ï¼Œè¯·æä¾›è®­ç»ƒæ•°æ®é›†")

        # æ£€æŸ¥æ˜¯å¦å·²ç» tokenized
        if "input_ids" in ds.column_names:
            print("âœ… æ•°æ®é›†å·²åŒ…å« input_idsï¼Œè·³è¿‡ tokenization")
            return ds

        if "prompt" not in ds.column_names:
            raise ValueError(
                "æ•°æ®é›†å¿…é¡»åŒ…å« 'input_ids' æˆ– 'prompt' åˆ—ã€‚"
                "è¯·ä½¿ç”¨ create_rl_dataset() åˆ›å»ºæ•°æ®é›†ï¼Œæˆ–æ‰‹åŠ¨æ·»åŠ  'prompt' åˆ—ã€‚"
            )

        if self.tokenizer is None:
            raise ValueError("tokenizer å°šæœªåˆå§‹åŒ–ï¼Œè¯·å…ˆè°ƒç”¨ setup_model()")

        print("ğŸ“ æ­£åœ¨ tokenize æ•°æ®é›†...")
        tokenizer = self.tokenizer

        def tokenize_fn(examples):
            return tokenizer(
                examples["prompt"],
                padding="max_length",
                truncation=True,
                max_length=self.config.max_length,
                return_tensors=None,
            )

        tokenized_ds = ds.map(
            tokenize_fn,
            batched=True,
            remove_columns=[
                col for col in ds.column_names if col not in ("input_ids", "attention_mask")
            ],
        )
        tokenized_ds.set_format(type="torch")
        print(f"âœ… Tokenization å®Œæˆï¼Œå…± {len(tokenized_ds)} æ¡æ ·æœ¬")
        return tokenized_ds

    def train(self):
        """å¼€å§‹PPOè®­ç»ƒ

        å®Œæ•´æµç¨‹ï¼š
        1. åŠ è½½ç­–ç•¥æ¨¡å‹ã€å¥–åŠ±æ¨¡å‹ã€ä»·å€¼æ¨¡å‹
        2. å‡†å¤‡æ•°æ®é›†ï¼ˆtokenizeï¼‰
        3. é…ç½® PPOConfig
        4. åˆ›å»º PPOTrainer å¹¶å¼€å§‹è®­ç»ƒ

        Returns:
            PPOTrainer å®ä¾‹
        """
        import os
        from trl import PPOConfig, PPOTrainer

        # é™é»˜å®éªŒæ€§è­¦å‘Š
        os.environ["TRL_EXPERIMENTAL_SILENCE"] = "1"

        if self.model is None:
            self.setup_model()

        if self.dataset is None:
            raise ValueError("æ•°æ®é›†æœªè®¾ç½®ï¼Œè¯·æä¾›è®­ç»ƒæ•°æ®é›†")

        # å‡†å¤‡ tokenized æ•°æ®é›†
        tokenized_dataset = self.prepare_dataset()

        # ç¡®å®š report_to å‚æ•°
        report_to = []
        if self.config.use_wandb:
            report_to.append("wandb")
        if self.config.use_tensorboard:
            report_to.append("tensorboard")
        if not report_to:
            report_to = ["none"]

        # é…ç½®PPOè®­ç»ƒå‚æ•°
        training_args = PPOConfig(
            output_dir=self.config.output_dir,
            num_train_epochs=self.config.num_train_epochs,
            per_device_train_batch_size=self.config.per_device_train_batch_size,
            gradient_accumulation_steps=self.config.gradient_accumulation_steps,
            learning_rate=self.config.learning_rate,
            warmup_steps=self.config.warmup_steps,
            logging_steps=self.config.logging_steps,
            save_steps=self.config.save_steps,
            fp16=self.config.use_fp16,
            bf16=self.config.use_bf16,
            report_to=report_to,
            # PPO ç‰¹å®šå‚æ•°
            num_ppo_epochs=4,
            kl_coef=0.05,
            cliprange=0.2,
            vf_coef=0.1,
            cliprange_value=0.2,
            gamma=1.0,
            lam=0.95,
            whiten_rewards=False,
            # ç”Ÿæˆå‚æ•°
            response_length=self.config.max_new_tokens,
            temperature=self.config.temperature,
            stop_token="eos",
            # éœ€è¦ eval_dataset æ‰èƒ½ä½¿ç”¨ num_sample_generations
            num_sample_generations=0,
        )

        # åˆ›å»ºè¯¦ç»†æ—¥å¿—å›è°ƒ
        logging_callback = DetailedLoggingCallback(
            num_epochs=self.config.num_train_epochs
        )

        # åˆ›å»ºPPOè®­ç»ƒå™¨
        self.trainer = PPOTrainer(
            args=training_args,
            processing_class=self.tokenizer,
            model=self.model,
            ref_model=self.ref_model,
            reward_model=self.reward_model,
            value_model=self.value_model,
            train_dataset=tokenized_dataset,
            callbacks=[logging_callback],
            peft_config=self.peft_config,
        )

        print("\nğŸš€ å¼€å§‹PPOè®­ç»ƒ...")
        print(f"{'='*80}\n")
        self.trainer.train()
        print(f"\n{'='*80}")
        print("âœ… PPOè®­ç»ƒå®Œæˆ")

        return self.trainer

