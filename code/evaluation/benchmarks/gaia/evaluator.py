"""
GAIA è¯„ä¼°å™¨æ¨¡å—

è´Ÿè´£è¯„ä¼°æ™ºèƒ½ä½“åœ¨ GAIA åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°
"""

from typing import Dict, Any, List, Optional, Union, Tuple
import time
import re
import json
import logging
import base64
from pathlib import Path
from code.evaluation.benchmarks.gaia.dataset import GAIADataset
from code.evaluation.benchmarks.gaia.metrics import GAIAMetrics

logger = logging.getLogger(__name__)

# Evaluation prompts directory (benchmark-specific prompts)
EVAL_PROMPTS_DIR = Path(__file__).resolve().parent.parent.parent / "prompts"


def _load_prompt(path: Path, fallback: str | None = None) -> str | None:
    """Load a prompt file, returning *fallback* if the file is missing or empty."""
    try:
        text = path.read_text(encoding="utf-8").strip()
        return text or fallback
    except FileNotFoundError:
        logger.warning("Prompt file not found: %s", path)
        return fallback


class GAIAEvaluator:
    """GAIA è¯„ä¼°å™¨

    è¯„ä¼°æ™ºèƒ½ä½“çš„é€šç”¨AIåŠ©æ‰‹èƒ½åŠ›,åŒ…æ‹¬:
    - é—®é¢˜ç†è§£å’Œæ¨ç†
    - å¤šæ­¥éª¤é—®é¢˜è§£å†³
    - å·¥å…·ä½¿ç”¨èƒ½åŠ›
    - ç­”æ¡ˆå‡†ç¡®æ€§

    GAIAè¯„ä¼°é‡‡ç”¨ä¸¥æ ¼çš„ç­”æ¡ˆåŒ¹é…æ ‡å‡†:
    - ç²¾ç¡®åŒ¹é…: ç­”æ¡ˆå®Œå…¨ä¸€è‡´
    - éƒ¨åˆ†åŒ¹é…: ç­”æ¡ˆåŒ…å«æ­£ç¡®ä¿¡æ¯ä½†æ ¼å¼ä¸åŒ

    Attributes:
        dataset: GAIA æ•°æ®é›†
        metrics: è¯„ä¼°æŒ‡æ ‡è®¡ç®—å™¨
        level: éš¾åº¦çº§åˆ«
        strict_mode: æ˜¯å¦ä½¿ç”¨ä¸¥æ ¼åŒ¹é…æ¨¡å¼
    """

    def __init__(
        self,
        dataset: Optional[GAIADataset] = None,
        level: Optional[int] = None,
        local_data_dir: Optional[str] = None,
        strict_mode: bool = True,
        llm=None,
    ):
        """åˆå§‹åŒ– GAIA è¯„ä¼°å™¨

        Args:
            dataset: GAIA æ•°æ®é›†,å¦‚æœä¸º None åˆ™è‡ªåŠ¨åˆ›å»º
            level: éš¾åº¦çº§åˆ« (1-3)
            local_data_dir: æœ¬åœ°æ•°æ®ç›®å½•
            strict_mode: æ˜¯å¦ä½¿ç”¨ä¸¥æ ¼åŒ¹é…æ¨¡å¼
            llm: Optional HelloAgentsLLM instance for direct LLM mode
                 (bypasses the ReAct agent loop).
        """
        self.dataset = dataset or GAIADataset(
            level=level,
            local_data_dir=local_data_dir
        )
        self.metrics = GAIAMetrics()
        self.level = level
        self.strict_mode = strict_mode
        self.llm = llm

        # Load benchmark-specific prompts
        self.gaia_system_prompt = _load_prompt(EVAL_PROMPTS_DIR / "gaia_system.prompt")
        self.task_template = _load_prompt(EVAL_PROMPTS_DIR / "gaia_task.prompt")
        
    def evaluate(self, agent: Any = None, max_samples: Optional[int] = None) -> Dict[str, Any]:
        """è¯„ä¼°æ™ºèƒ½ä½“

        Args:
            agent: è¦è¯„ä¼°çš„æ™ºèƒ½ä½“ (used only when self.llm is None)
            max_samples: æœ€å¤§è¯„ä¼°æ ·æœ¬æ•°,Noneè¡¨ç¤ºè¯„ä¼°å…¨éƒ¨

        Returns:
            è¯„ä¼°ç»“æœå­—å…¸,åŒ…å«å„é¡¹æŒ‡æ ‡
        """
        mode_label = "direct LLM" if self.llm else "agent"
        agent_name = getattr(self.llm, 'model', None) or getattr(agent, 'name', 'Unknown')

        print(f"\nğŸŒŸ å¼€å§‹ GAIA è¯„ä¼°...")
        print(f"   æ¨¡å¼: {mode_label}")
        print(f"   æ¨¡å‹/æ™ºèƒ½ä½“: {agent_name}")
        print(f"   éš¾åº¦çº§åˆ«: {self.level or 'å…¨éƒ¨'}")
        print(f"   åŒ¹é…æ¨¡å¼: {'ä¸¥æ ¼' if self.strict_mode else 'å®½æ¾'}")

        # åŠ è½½æ•°æ®é›†
        dataset = self.dataset.load()
        if not dataset:
            print("   âš ï¸ æ•°æ®é›†ä¸ºç©º,è·³è¿‡è¯„ä¼°")
            return self._create_empty_results(agent)

        # é™åˆ¶æ ·æœ¬æ•°é‡
        if max_samples:
            dataset = dataset[:max_samples]

        print(f"   æ ·æœ¬æ•°é‡: {len(dataset)}")

        # æ‰§è¡Œè¯„ä¼°
        results = []
        skipped_samples = 0
        level_stats = {1: {"total": 0, "correct": 0, "partial": 0},
                      2: {"total": 0, "correct": 0, "partial": 0},
                      3: {"total": 0, "correct": 0, "partial": 0}}

        for i, sample in enumerate(dataset):
            if i % 10 == 0:
                print(f"   è¿›åº¦: {i+1}/{len(dataset)}")

            try:
                sample_result = self.evaluate_sample(agent, sample)
                results.append(sample_result)

                # Track skipped samples
                if sample_result.get("skipped"):
                    skipped_samples += 1
                    continue

                # æŒ‰çº§åˆ«ç»Ÿè®¡
                level = sample.get("level", 1)
                if level in level_stats:
                    level_stats[level]["total"] += 1
                    if sample_result["exact_match"]:
                        level_stats[level]["correct"] += 1
                    if sample_result["partial_match"]:
                        level_stats[level]["partial"] += 1

            except Exception as e:
                print(f"   âš ï¸ æ ·æœ¬ {i} è¯„ä¼°å¤±è´¥: {e}")
                results.append({
                    "exact_match": False,
                    "partial_match": False,
                    "predicted": None,
                    "expected": sample.get("final_answer"),
                    "error": str(e),
                    "score": 0.0
                })

        # è®¡ç®—æ€»ä½“æŒ‡æ ‡ (skipped samples excluded from totals)
        evaluated_results = [r for r in results if not r.get("skipped")]
        total_samples = len(evaluated_results)
        exact_matches = sum(1 for r in evaluated_results if r["exact_match"])
        partial_matches = sum(1 for r in evaluated_results if r["partial_match"])

        exact_match_rate = exact_matches / total_samples if total_samples > 0 else 0.0
        partial_match_rate = partial_matches / total_samples if total_samples > 0 else 0.0

        # è®¡ç®—åˆ†çº§æŒ‡æ ‡
        level_metrics = {}
        for level, stats in level_stats.items():
            if stats["total"] > 0:
                level_metrics[f"Level_{level}"] = {
                    "total": stats["total"],
                    "exact_matches": stats["correct"],
                    "partial_matches": stats["partial"],
                    "exact_match_rate": stats["correct"] / stats["total"],
                    "partial_match_rate": stats["partial"] / stats["total"]
                }

        final_results = {
            "benchmark": "GAIA",
            "agent_name": agent_name,
            "strict_mode": self.strict_mode,
            "level_filter": self.level,
            "total_samples": total_samples,
            "skipped_samples": skipped_samples,
            "exact_matches": exact_matches,
            "partial_matches": partial_matches,
            "exact_match_rate": exact_match_rate,
            "partial_match_rate": partial_match_rate,
            "level_metrics": level_metrics,
            "detailed_results": results
        }

        print(f"âœ… GAIA è¯„ä¼°å®Œæˆ")
        print(f"   è¯„ä¼°æ ·æœ¬: {total_samples} (è·³è¿‡: {skipped_samples})")
        print(f"   ç²¾ç¡®åŒ¹é…ç‡: {exact_match_rate:.2%}")
        print(f"   éƒ¨åˆ†åŒ¹é…ç‡: {partial_match_rate:.2%}")
        for level_name, metrics in level_metrics.items():
            print(f"   {level_name}: {metrics['exact_match_rate']:.2%} ç²¾ç¡® / {metrics['partial_match_rate']:.2%} éƒ¨åˆ†")

        return final_results
    
    def evaluate_sample(self, agent: Any, sample: Dict[str, Any]) -> Dict[str, Any]:
        """è¯„ä¼°å•ä¸ªæ ·æœ¬

        Uses direct LLM invocation when self.llm is set, otherwise falls back
        to agent.run().  Loads file attachments and includes their content
        in the prompt (text) or as multimodal image blocks (LLM mode only).

        Args:
            agent: è¦è¯„ä¼°çš„æ™ºèƒ½ä½“ (ignored when self.llm is set)
            sample: æ ·æœ¬æ•°æ®

        Returns:
            å•ä¸ªæ ·æœ¬çš„è¯„ä¼°ç»“æœ
        """
        try:
            # å‡†å¤‡è¾“å…¥
            question = sample.get("question", "")
            expected_answer = sample.get("final_answer", "")
            level = sample.get("level", 1)
            task_id = sample.get("task_id", "")

            # Load file attachment
            file_name = sample.get("file_name", "")
            content_type, file_content = self._load_file_content(file_name) if file_name else (None, None)

            # Skip unsupported file types
            if file_name and content_type is None and file_content is None:
                print(f"   â­ï¸ è·³è¿‡æ ·æœ¬ {task_id}: ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹ ({Path(file_name).suffix})")
                return {
                    "task_id": task_id,
                    "level": level,
                    "exact_match": False,
                    "partial_match": False,
                    "score": 0.0,
                    "predicted": None,
                    "expected": expected_answer,
                    "skipped": True,
                    "skip_reason": f"Unsupported file type: {Path(file_name).suffix}",
                }

            # Build prompt with file text content (if applicable)
            file_text = file_content if content_type == "text" else None
            prompt = self._build_prompt(question, sample, file_text=file_text)

            start_time = time.time()

            if self.llm:
                # Direct LLM mode â€” prepend system prompt if available
                messages = []
                if self.gaia_system_prompt:
                    messages.append({"role": "system", "content": self.gaia_system_prompt})
                if content_type == "image":
                    messages.extend(self._build_multimodal_messages(prompt, file_content, file_name))
                else:
                    messages.append({"role": "user", "content": prompt})
                response = self.llm.invoke(messages)
            else:
                # Agent mode â€” images not supported (agent.run takes a string)
                if content_type == "image":
                    print(f"   â­ï¸ è·³è¿‡æ ·æœ¬ {task_id}: agentæ¨¡å¼ä¸æ”¯æŒå›¾ç‰‡æ–‡ä»¶")
                    return {
                        "task_id": task_id,
                        "level": level,
                        "exact_match": False,
                        "partial_match": False,
                        "score": 0.0,
                        "predicted": None,
                        "expected": expected_answer,
                        "skipped": True,
                        "skip_reason": "Image files not supported in agent mode",
                    }
                response = agent.run(prompt)

            execution_time = time.time() - start_time

            # æå–ç­”æ¡ˆ
            predicted_answer = self._extract_answer(response)

            # è¯„ä¼°ç­”æ¡ˆ
            exact_match = self._check_exact_match(predicted_answer, expected_answer)
            partial_match = self._check_partial_match(predicted_answer, expected_answer)

            # è®¡ç®—åˆ†æ•°
            if exact_match:
                score = 1.0
            elif partial_match:
                score = 0.5
            else:
                score = 0.0

            return {
                "task_id": task_id,
                "level": level,
                "exact_match": exact_match,
                "partial_match": partial_match,
                "score": score,
                "predicted": predicted_answer,
                "expected": expected_answer,
                "response": response,
                "execution_time": execution_time
            }

        except Exception as e:
            return {
                "task_id": sample.get("task_id", ""),
                "level": sample.get("level", 1),
                "exact_match": False,
                "partial_match": False,
                "score": 0.0,
                "predicted": None,
                "expected": sample.get("final_answer", ""),
                "error": str(e)
            }

    def _create_empty_results(self, agent: Any) -> Dict[str, Any]:
        """åˆ›å»ºç©ºçš„è¯„ä¼°ç»“æœ"""
        agent_name = getattr(self.llm, 'model', None) or getattr(agent, 'name', 'Unknown')
        return {
            "benchmark": "GAIA",
            "agent_name": agent_name,
            "strict_mode": self.strict_mode,
            "level_filter": self.level,
            "total_samples": 0,
            "skipped_samples": 0,
            "exact_matches": 0,
            "partial_matches": 0,
            "exact_match_rate": 0.0,
            "partial_match_rate": 0.0,
            "level_metrics": {},
            "detailed_results": []
        }

    def _build_prompt(self, question: str, sample: Dict[str, Any], file_text: Optional[str] = None) -> str:
        """æ„å»ºè¯„ä¼°æç¤º

        Uses the gaia_task.prompt template if available, otherwise falls back
        to the original inline format.

        Args:
            question: é—®é¢˜æ–‡æœ¬
            sample: æ ·æœ¬æ•°æ®
            file_text: å·²åŠ è½½çš„æ–‡ä»¶æ–‡æœ¬å†…å®¹ (for text-type attachments)
        """
        if self.task_template:
            file_section = ""
            if file_text and sample.get("file_name"):
                file_basename = Path(sample["file_name"]).name
                file_section = f"\n## Attached File: {file_basename}\n\n{file_text}\n"
            elif sample.get("file_name") and not file_text:
                file_section = f"\nNote: This question may require reference to the file: {Path(sample['file_name']).name}\n"
            return self.task_template.format(
                question=question,
                file_section=file_section,
            )

        # Fallback: original inline prompt
        prompt = f"{question}"

        if file_text and sample.get("file_name"):
            file_basename = Path(sample["file_name"]).name
            prompt += f"\n\n## Attached File: {file_basename}\n\n{file_text}"
        elif sample.get("file_name") and not file_text:
            prompt += f"\n\nNote: This question may require reference to the file: {Path(sample['file_name']).name}"

        return prompt

    def _load_file_content(self, file_path: str) -> Tuple[Optional[str], Optional[str]]:
        """Read a file from disk and return its content for prompt inclusion.

        Returns:
            (content_type, content) where content_type is "text", "image", or
            None (unsupported).  For text, content is the string.  For image,
            content is a base64 data-URL.
        """
        if not file_path:
            return None, None

        path = Path(file_path)
        if not path.exists():
            print(f"   âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return None, None

        suffix = path.suffix.lower()

        # --- Text-readable formats ---
        text_extensions = {
            ".txt", ".md", ".py", ".csv", ".json", ".jsonl",
            ".xml", ".html", ".log", ".tsv", ".yaml", ".yml",
        }
        if suffix in text_extensions:
            try:
                content = path.read_text(encoding="utf-8")
                # Truncate very large files to avoid blowing up the context
                max_chars = 50_000
                if len(content) > max_chars:
                    content = content[:max_chars] + "\n\n... [truncated]"
                return "text", content
            except Exception as e:
                print(f"   âš ï¸ æ–‡ä»¶è¯»å–å¤±è´¥ ({path.name}): {e}")
                return None, None

        # --- Excel ---
        if suffix in (".xlsx", ".xls"):
            try:
                import openpyxl
                wb = openpyxl.load_workbook(str(path), read_only=True, data_only=True)
                text_parts = []
                for sheet_name in wb.sheetnames:
                    ws = wb[sheet_name]
                    text_parts.append(f"### Sheet: {sheet_name}\n")
                    for row in ws.iter_rows(values_only=True):
                        row_str = "\t".join(str(cell) if cell is not None else "" for cell in row)
                        text_parts.append(row_str)
                wb.close()
                content = "\n".join(text_parts)
                max_chars = 50_000
                if len(content) > max_chars:
                    content = content[:max_chars] + "\n\n... [truncated]"
                return "text", content
            except ImportError:
                print("   âš ï¸ openpyxlæœªå®‰è£…, æ— æ³•è¯»å–Excelæ–‡ä»¶")
                return None, None
            except Exception as e:
                print(f"   âš ï¸ Excelè¯»å–å¤±è´¥ ({path.name}): {e}")
                return None, None

        # --- PDF ---
        if suffix == ".pdf":
            try:
                import pypdf
                reader = pypdf.PdfReader(str(path))
                pages_text = []
                for page in reader.pages:
                    text = page.extract_text()
                    if text:
                        pages_text.append(text)
                content = "\n\n".join(pages_text)
                max_chars = 50_000
                if len(content) > max_chars:
                    content = content[:max_chars] + "\n\n... [truncated]"
                return "text", content
            except ImportError:
                try:
                    import PyPDF2
                    reader = PyPDF2.PdfReader(str(path))
                    pages_text = []
                    for page in reader.pages:
                        text = page.extract_text()
                        if text:
                            pages_text.append(text)
                    content = "\n\n".join(pages_text)
                    max_chars = 50_000
                    if len(content) > max_chars:
                        content = content[:max_chars] + "\n\n... [truncated]"
                    return "text", content
                except ImportError:
                    print("   âš ï¸ pypdf/PyPDF2æœªå®‰è£…, æ— æ³•è¯»å–PDFæ–‡ä»¶")
                    return None, None
            except Exception as e:
                print(f"   âš ï¸ PDFè¯»å–å¤±è´¥ ({path.name}): {e}")
                return None, None

        # --- Image ---
        image_extensions = {".png", ".jpg", ".jpeg", ".gif", ".webp", ".bmp"}
        if suffix in image_extensions:
            try:
                mime_map = {
                    ".png": "image/png",
                    ".jpg": "image/jpeg",
                    ".jpeg": "image/jpeg",
                    ".gif": "image/gif",
                    ".webp": "image/webp",
                    ".bmp": "image/bmp",
                }
                mime = mime_map.get(suffix, "image/png")
                raw = path.read_bytes()
                b64 = base64.b64encode(raw).decode("ascii")
                data_url = f"data:{mime};base64,{b64}"
                return "image", data_url
            except Exception as e:
                print(f"   âš ï¸ å›¾ç‰‡è¯»å–å¤±è´¥ ({path.name}): {e}")
                return None, None

        # --- Unsupported ---
        print(f"   âš ï¸ ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {suffix}")
        return None, None

    def _build_multimodal_messages(self, prompt: str, image_data_url: str, file_name: str) -> List[Dict]:
        """Construct OpenAI-compatible multimodal messages with an image.

        Args:
            prompt: The text prompt.
            image_data_url: Base64 data URL for the image.
            file_name: Original file name (for logging context).

        Returns:
            Messages list suitable for HelloAgentsLLM.invoke().
        """
        return [{
            "role": "user",
            "content": [
                {"type": "text", "text": prompt},
                {"type": "image_url", "image_url": {"url": image_data_url}},
            ]
        }]

    def _extract_answer(self, response: str) -> str:
        """ä»å“åº”ä¸­æå–ç­”æ¡ˆï¼ˆGAIAæ ¼å¼ï¼‰

        GAIAè¦æ±‚ç­”æ¡ˆæ ¼å¼ä¸ºï¼šFINAL ANSWER: [ç­”æ¡ˆ]
        """
        # é¦–å…ˆå°è¯•æå–GAIAå®˜æ–¹æ ¼å¼çš„ç­”æ¡ˆ
        final_answer_pattern = r'FINAL ANSWER:\s*(.+?)(?:\n|$)'
        match = re.search(final_answer_pattern, response, re.IGNORECASE | re.MULTILINE)
        if match:
            answer = match.group(1).strip()
            # ç§»é™¤å¯èƒ½çš„æ–¹æ‹¬å·
            answer = answer.strip('[]')
            return answer

        # å¤‡ç”¨æ–¹æ¡ˆï¼šæŸ¥æ‰¾å…¶ä»–ç­”æ¡ˆæ ‡è®°
        answer_patterns = [
            r'ç­”æ¡ˆ[ï¼š:]\s*(.+)',
            r'æœ€ç»ˆç­”æ¡ˆ[ï¼š:]\s*(.+)',
            r'Final answer[ï¼š:]\s*(.+)',
            r'Answer[ï¼š:]\s*(.+)',
        ]

        for pattern in answer_patterns:
            match = re.search(pattern, response, re.IGNORECASE)
            if match:
                return match.group(1).strip()

        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ ‡è®°ï¼Œè¿”å›æœ€åä¸€ä¸ªéç©ºè¡Œ
        lines = response.strip().split('\n')
        for line in reversed(lines):
            line = line.strip()
            if line and not line.startswith('#'):
                return line

        return response.strip()

    def _check_exact_match(self, predicted: str, expected: str) -> bool:
        """æ£€æŸ¥ç²¾ç¡®åŒ¹é…"""
        if not predicted or not expected:
            return False

        # æ ‡å‡†åŒ–å­—ç¬¦ä¸²
        pred_normalized = self._normalize_answer(predicted)
        exp_normalized = self._normalize_answer(expected)

        return pred_normalized == exp_normalized

    def _check_partial_match(self, predicted: str, expected: str) -> bool:
        """æ£€æŸ¥éƒ¨åˆ†åŒ¹é…"""
        if not predicted or not expected:
            return False

        # æ ‡å‡†åŒ–å­—ç¬¦ä¸²
        pred_normalized = self._normalize_answer(predicted)
        exp_normalized = self._normalize_answer(expected)

        # æ£€æŸ¥åŒ…å«å…³ç³»
        if exp_normalized in pred_normalized or pred_normalized in exp_normalized:
            return True

        # æ£€æŸ¥å…³é”®è¯åŒ¹é…
        pred_words = set(pred_normalized.split())
        exp_words = set(exp_normalized.split())

        if not exp_words:
            return False

        # å¦‚æœè¶…è¿‡70%çš„æœŸæœ›è¯æ±‡å‡ºç°åœ¨é¢„æµ‹ä¸­ï¼Œè®¤ä¸ºéƒ¨åˆ†åŒ¹é…
        overlap = len(pred_words & exp_words)
        return overlap / len(exp_words) >= 0.7

    def _normalize_answer(self, answer: str) -> str:
        """æ ‡å‡†åŒ–ç­”æ¡ˆå­—ç¬¦ä¸²ï¼ˆGAIAå®˜æ–¹æ ‡å‡†åŒ–è§„åˆ™ï¼‰

        æ ¹æ®GAIAè®ºæ–‡çš„æ ‡å‡†åŒ–è§„åˆ™ï¼š
        1. æ•°å­—ï¼šç§»é™¤é€—å·åˆ†éš”ç¬¦å’Œå•ä½ç¬¦å·
        2. å­—ç¬¦ä¸²ï¼šç§»é™¤å† è¯ã€è½¬å°å†™ã€ç§»é™¤å¤šä½™ç©ºæ ¼
        3. åˆ—è¡¨ï¼šæŒ‰é€—å·åˆ†éš”ï¼Œæ¯ä¸ªå…ƒç´ ç‹¬ç«‹æ ‡å‡†åŒ–
        """
        if not answer:
            return ""

        answer = answer.strip()

        # æ£€æŸ¥æ˜¯å¦æ˜¯é€—å·åˆ†éš”çš„åˆ—è¡¨
        if ',' in answer:
            # åˆ†éš”å¹¶æ ‡å‡†åŒ–æ¯ä¸ªå…ƒç´ 
            parts = [self._normalize_single_answer(p.strip()) for p in answer.split(',')]
            # æŒ‰å­—æ¯é¡ºåºæ’åºï¼ˆGAIAè¦æ±‚ï¼‰
            parts.sort()
            return ','.join(parts)
        else:
            return self._normalize_single_answer(answer)

    def _normalize_single_answer(self, answer: str) -> str:
        """æ ‡å‡†åŒ–å•ä¸ªç­”æ¡ˆï¼ˆä¸åŒ…å«é€—å·çš„ç­”æ¡ˆï¼‰"""
        answer = answer.strip().lower()

        # ç§»é™¤å¸¸è§çš„å† è¯
        articles = ['the', 'a', 'an']
        words = answer.split()
        if words and words[0] in articles:
            words = words[1:]
            answer = ' '.join(words)

        # ç§»é™¤è´§å¸ç¬¦å·å’Œç™¾åˆ†å·
        answer = answer.replace('$', '').replace('%', '').replace('â‚¬', '').replace('Â£', '')

        # ç§»é™¤æ•°å­—ä¸­çš„é€—å·åˆ†éš”ç¬¦ï¼ˆå¦‚ 1,000 -> 1000ï¼‰
        # ä½†ä¿ç•™å°æ•°ç‚¹
        answer = re.sub(r'(\d),(\d)', r'\1\2', answer)

        # ç§»é™¤å¤šä½™ç©ºæ ¼
        answer = ' '.join(answer.split())

        # ç§»é™¤æœ«å°¾çš„æ ‡ç‚¹ç¬¦å·
        answer = answer.rstrip('.,;:!?')

        return answer

    def export_to_gaia_format(
        self,
        results: Dict[str, Any],
        output_path: Union[str, Path],
        include_reasoning: bool = True
    ) -> None:
        """å¯¼å‡ºä¸ºGAIAå®˜æ–¹æ ¼å¼

        GAIAæ ¼å¼è¦æ±‚ï¼š
        - JSONLæ ¼å¼ï¼ˆæ¯è¡Œä¸€ä¸ªJSONå¯¹è±¡ï¼‰
        - æ¯ä¸ªå¯¹è±¡åŒ…å«ï¼štask_id, model_answer, reasoning_traceï¼ˆå¯é€‰ï¼‰

        Args:
            results: è¯„ä¼°ç»“æœ
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            include_reasoning: æ˜¯å¦åŒ…å«æ¨ç†è½¨è¿¹
        """
        output_path = Path(output_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)

        detailed_results = results.get("detailed_results", [])

        with open(output_path, 'w', encoding='utf-8') as f:
            for result in detailed_results:
                gaia_result = {
                    "task_id": result.get("task_id", ""),
                    "model_answer": result.get("predicted", "")
                }

                if include_reasoning:
                    gaia_result["reasoning_trace"] = result.get("response", "")

                f.write(json.dumps(gaia_result, ensure_ascii=False) + '\n')

        print(f"âœ… GAIAæ ¼å¼ç»“æœå·²å¯¼å‡º")
        print(f"   è¾“å‡ºæ–‡ä»¶: {output_path}")
        print(f"   æ ·æœ¬æ•°: {len(detailed_results)}")
        print(f"   åŒ…å«æ¨ç†è½¨è¿¹: {include_reasoning}")

