"""
SWE-bench Evaluation Metrics Module

Computes SWE-bench evaluation metrics including patch matching and per-repo aggregation.
"""

from typing import Dict, Any, List, Set
import re


class SWEMetrics:
    """SWE-bench evaluation metrics calculator

    Computes software engineering fix-related evaluation metrics:
    - Resolved Rate: Proportion of instances with patch match or passing tests
    - Patch Metrics: File-level and line-level overlap
    - Per-repo Metrics: Resolved rate grouped by repository
    """

    @staticmethod
    def calculate_resolved_rate(results: List[Dict[str, Any]]) -> float:
        """Compute the resolved rate.

        An instance is considered resolved if exact_match is True or tests_passed is True.

        Args:
            results: List of evaluation results for each instance

        Returns:
            Resolved rate (0-1)
        """
        if not results:
            return 0.0

        resolved = sum(
            1
            for r in results
            if r.get("exact_match", False) or r.get("tests_passed", False)
        )
        return resolved / len(results)

    @staticmethod
    def calculate_patch_metrics(
        predicted_patch: str, gold_patch: str
    ) -> Dict[str, Any]:
        """Compute matching metrics between two patches.

        Args:
            predicted_patch: Patch generated by the agent
            gold_patch: Gold standard patch

        Returns:
            Metrics dict containing exact_match / files_matched / line_overlap
        """
        exact_match = predicted_patch.strip() == gold_patch.strip()

        # If raw diff strings differ (e.g., due to index/hash header lines),
        # fall back to semantic comparison: same files + same changed lines.
        if not exact_match and predicted_patch.strip() and gold_patch.strip():
            pred_sem = SWEMetrics._normalize_patch(predicted_patch)
            gold_sem = SWEMetrics._normalize_patch(gold_patch)
            exact_match = pred_sem == gold_sem

        pred_files = SWEMetrics._extract_changed_files(predicted_patch)
        gold_files = SWEMetrics._extract_changed_files(gold_patch)

        if gold_files:
            file_intersection = pred_files & gold_files
            file_precision = len(file_intersection) / len(pred_files) if pred_files else 0.0
            file_recall = len(file_intersection) / len(gold_files)
        else:
            file_precision = 0.0
            file_recall = 0.0

        files_matched = pred_files == gold_files

        pred_lines = SWEMetrics._extract_changed_lines(predicted_patch)
        gold_lines = SWEMetrics._extract_changed_lines(gold_patch)

        if pred_lines or gold_lines:
            intersection = pred_lines & gold_lines
            union = pred_lines | gold_lines
            line_overlap = len(intersection) / len(union) if union else 0.0
        else:
            line_overlap = 1.0 if exact_match else 0.0

        return {
            "exact_match": exact_match,
            "files_matched": files_matched,
            "file_precision": file_precision,
            "file_recall": file_recall,
            "line_overlap": line_overlap,
            "predicted_files": sorted(pred_files),
            "gold_files": sorted(gold_files),
        }

    @staticmethod
    def calculate_repo_metrics(results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Compute metrics grouped by repository.

        Args:
            results: List of evaluation results for each instance

        Returns:
            Dict keyed by repository name with per-repo metrics
        """
        repo_groups: Dict[str, List[Dict[str, Any]]] = {}
        for r in results:
            repo = r.get("repo", "unknown")
            repo_groups.setdefault(repo, []).append(r)

        repo_metrics: Dict[str, Any] = {}
        for repo, group in sorted(repo_groups.items()):
            resolved = sum(
                1
                for r in group
                if r.get("exact_match", False) or r.get("tests_passed", False)
            )
            repo_metrics[repo] = {
                "total": len(group),
                "resolved": resolved,
                "resolved_rate": resolved / len(group) if group else 0.0,
            }

        return repo_metrics

    def compute_metrics(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Compute aggregate metrics.

        Args:
            results: List of evaluation results

        Returns:
            Complete metrics dictionary
        """
        if not results:
            return self._empty_metrics()

        resolved_rate = self.calculate_resolved_rate(results)
        repo_metrics = self.calculate_repo_metrics(results)

        # Aggregate patch metrics
        exact_matches = sum(1 for r in results if r.get("exact_match", False))
        files_matched = sum(
            1 for r in results if r.get("patch_metrics", {}).get("files_matched", False)
        )
        line_overlaps = [
            r.get("patch_metrics", {}).get("line_overlap", 0.0) for r in results
        ]
        avg_line_overlap = (
            sum(line_overlaps) / len(line_overlaps) if line_overlaps else 0.0
        )

        # Execution time statistics
        exec_times = [
            r.get("execution_time", 0.0) for r in results if "execution_time" in r
        ]
        avg_execution_time = sum(exec_times) / len(exec_times) if exec_times else 0.0

        # Finish reason counts
        finish_reason_counts: Dict[str, int] = {}
        for r in results:
            reason = r.get("finish_reason", "unknown")
            finish_reason_counts[reason] = finish_reason_counts.get(reason, 0) + 1

        # Average steps used
        steps_list = [
            r.get("steps_used", 0) for r in results if "steps_used" in r
        ]
        average_steps_used = sum(steps_list) / len(steps_list) if steps_list else 0.0

        return {
            "total_samples": len(results),
            "resolved_rate": resolved_rate,
            "exact_matches": exact_matches,
            "exact_match_rate": exact_matches / len(results),
            "files_matched_count": files_matched,
            "average_line_overlap": avg_line_overlap,
            "average_execution_time": avg_execution_time,
            "repo_metrics": repo_metrics,
            "finish_reason_counts": finish_reason_counts,
            "average_steps_used": average_steps_used,
        }

    def _empty_metrics(self) -> Dict[str, Any]:
        """Return empty metrics."""
        return {
            "total_samples": 0,
            "resolved_rate": 0.0,
            "exact_matches": 0,
            "exact_match_rate": 0.0,
            "files_matched_count": 0,
            "average_line_overlap": 0.0,
            "average_execution_time": 0.0,
            "repo_metrics": {},
            "finish_reason_counts": {},
            "average_steps_used": 0.0,
        }

    # ------------------------------------------------------------------
    # Internal helpers
    # ------------------------------------------------------------------

    @staticmethod
    def _extract_changed_files(patch: str) -> Set[str]:
        """Extract modified file paths from a unified-diff patch."""
        files: Set[str] = set()
        for m in re.finditer(r"^diff --git a/(.+?) b/(.+?)$", patch, re.MULTILINE):
            files.add(m.group(2))
        return files

    @staticmethod
    def _extract_changed_lines(patch: str) -> Set[str]:
        """Extract all added/removed lines from a patch (stripped of leading whitespace).

        Used for computing Jaccard similarity.
        """
        lines: Set[str] = set()
        for line in patch.splitlines():
            stripped = line.rstrip()
            if stripped.startswith("+") and not stripped.startswith("+++"):
                lines.add(stripped)
            elif stripped.startswith("-") and not stripped.startswith("---"):
                lines.add(stripped)
        return lines

    @staticmethod
    def _normalize_patch(patch: str) -> str:
        """Normalize a unified diff for semantic comparison.

        Strips header lines (diff, index, ---/+++) and keeps only
        the hunk headers (@@) and content lines (+/-/context).
        This allows comparison of patches that differ only in
        metadata (commit hashes, index lines, etc.).
        """
        normalized_lines = []
        for line in patch.strip().splitlines():
            # Keep hunk headers and content lines, skip metadata headers
            if line.startswith("@@") or \
               line.startswith("+") or \
               line.startswith("-") or \
               line.startswith(" "):
                # Skip --- and +++ header lines
                if line.startswith("---") or line.startswith("+++"):
                    continue
                normalized_lines.append(line.rstrip())
        return "\n".join(normalized_lines)
