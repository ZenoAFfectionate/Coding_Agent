"""
SWE-bench è¯„ä¼°å™¨æ¨¡å—

è´Ÿè´£è¯„ä¼°æ™ºèƒ½ä½“åœ¨ SWE-bench åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ã€‚

æ¯ä¸ªå®ä¾‹ä»£è¡¨ä¸€ä¸ªçœŸå®çš„ GitHub Issueï¼Œè¯„ä¼°å™¨ä¼š:
1. å…‹éš†å¯¹åº”ä»“åº“å¹¶åˆ‡æ¢åˆ° base_commit
2. æ„å»º promptï¼ˆissue + hintsï¼‰
3. è¿è¡Œæ™ºèƒ½ä½“
4. é€šè¿‡ git diff æ”¶é›†æ™ºèƒ½ä½“äº§ç”Ÿçš„è¡¥ä¸
5. å°†é¢„æµ‹è¡¥ä¸ä¸ gold patch è¿›è¡Œæ¯”è¾ƒï¼ˆå¯é€‰ï¼šè¿è¡Œæµ‹è¯•ï¼‰
"""

from typing import Dict, Any, List, Optional, Union, Callable
import time
import json
import subprocess
import tempfile
import shutil
from pathlib import Path

from code.evaluation.benchmarks.swe.dataset import SWEDataset
from code.evaluation.benchmarks.swe.metrics import SWEMetrics


class SWEEvaluator:
    """SWE-bench è¯„ä¼°å™¨

    è¯„ä¼°æ™ºèƒ½ä½“ä¿®å¤çœŸå® GitHub Issue çš„èƒ½åŠ›ã€‚

    Attributes:
        dataset: SWE-bench æ•°æ®é›†
        metrics: æŒ‡æ ‡è®¡ç®—å™¨
        workspace_base: ä¸´æ—¶å·¥ä½œç›®å½•çš„çˆ¶ç›®å½•
        timeout_per_instance: æ¯ä¸ªå®ä¾‹çš„è¶…æ—¶æ—¶é—´(ç§’)
        run_tests: æ˜¯å¦è¿è¡Œ FAIL_TO_PASS æµ‹è¯•
    """

    def __init__(
        self,
        dataset: Optional[SWEDataset] = None,
        workspace_base: Optional[str] = None,
        timeout_per_instance: int = 600,
        run_tests: bool = False,
    ):
        """åˆå§‹åŒ– SWE-bench è¯„ä¼°å™¨

        Args:
            dataset: SWE-bench æ•°æ®é›†ï¼Œä¸º None åˆ™è‡ªåŠ¨åˆ›å»º (dev split)
            workspace_base: å…‹éš†ä»“åº“çš„åŸºç¡€ç›®å½•ï¼Œé»˜è®¤ä½¿ç”¨ç³»ç»Ÿä¸´æ—¶ç›®å½•
            timeout_per_instance: æ¯ä¸ªå®ä¾‹çš„è¶…æ—¶æ—¶é—´(ç§’)
            run_tests: æ˜¯å¦æ‰§è¡Œ FAIL_TO_PASS æµ‹è¯•éªŒè¯
        """
        self.dataset = dataset if dataset is not None else SWEDataset()
        self.metrics = SWEMetrics()
        self.workspace_base = workspace_base
        self.timeout_per_instance = timeout_per_instance
        self.run_tests = run_tests

    def evaluate(
        self,
        agent_factory: Callable[..., Any],
        max_samples: Optional[int] = None,
        **agent_kwargs,
    ) -> Dict[str, Any]:
        """è¯„ä¼°æ™ºèƒ½ä½“

        å› ä¸ºæ¯ä¸ª SWE-bench å®ä¾‹éœ€è¦ä¸åŒçš„ workspaceï¼ˆå…‹éš†çš„ä»“åº“ï¼‰ï¼Œ
        æ‰€ä»¥ä¼ å…¥çš„æ˜¯ agent_factory (å¦‚ build_agent)ï¼Œè¯„ä¼°å™¨ä¼šä¸ºæ¯ä¸ª
        å®ä¾‹åˆ›å»ºæ–°çš„ agentã€‚

        Args:
            agent_factory: æ¥å— workspace å…³é”®å­—å‚æ•°å¹¶è¿”å› agent çš„å·¥å‚å‡½æ•°
            max_samples: æœ€å¤§è¯„ä¼°æ ·æœ¬æ•°ï¼ŒNone è¡¨ç¤ºè¯„ä¼°å…¨éƒ¨
            **agent_kwargs: ä¼ é€’ç»™ agent_factory çš„é¢å¤–å‚æ•°

        Returns:
            è¯„ä¼°ç»“æœå­—å…¸
        """
        print(f"\nğŸ”§ å¼€å§‹ SWE-bench è¯„ä¼°...")

        # åŠ è½½æ•°æ®é›†
        dataset = self.dataset.load()
        if not dataset:
            print("   âš ï¸ æ•°æ®é›†ä¸ºç©ºï¼Œè·³è¿‡è¯„ä¼°")
            return self._create_empty_results()

        # é™åˆ¶æ ·æœ¬æ•°é‡
        if max_samples:
            dataset = dataset[:max_samples]

        print(f"   æ ·æœ¬æ•°é‡: {len(dataset)}")
        print(f"   è¿è¡Œæµ‹è¯•: {'æ˜¯' if self.run_tests else 'å¦'}")

        results: List[Dict[str, Any]] = []
        for i, sample in enumerate(dataset):
            print(
                f"   è¿›åº¦: {i + 1}/{len(dataset)} - {sample.get('instance_id', '')}"
            )

            try:
                sample_result = self.evaluate_sample(
                    agent_factory, sample, **agent_kwargs
                )
                results.append(sample_result)
            except Exception as e:
                print(f"   âš ï¸ å®ä¾‹ {sample.get('instance_id')} è¯„ä¼°å¤±è´¥: {e}")
                results.append(
                    {
                        "instance_id": sample.get("instance_id", ""),
                        "repo": sample.get("repo", ""),
                        "exact_match": False,
                        "tests_passed": False,
                        "patch_metrics": {},
                        "predicted_patch": "",
                        "error": str(e),
                        "score": 0.0,
                    }
                )

        # è®¡ç®—ç»¼åˆæŒ‡æ ‡
        overall_metrics = self.metrics.compute_metrics(results)

        final_results = {
            "benchmark": "SWE-bench",
            "total_samples": len(results),
            "resolved_rate": overall_metrics["resolved_rate"],
            "exact_match_rate": overall_metrics["exact_match_rate"],
            "average_line_overlap": overall_metrics["average_line_overlap"],
            "average_execution_time": overall_metrics["average_execution_time"],
            "repo_metrics": overall_metrics["repo_metrics"],
            "detailed_results": results,
        }

        print(f"âœ… SWE-bench è¯„ä¼°å®Œæˆ")
        print(f"   è§£å†³ç‡: {overall_metrics['resolved_rate']:.2%}")
        print(f"   ç²¾ç¡®åŒ¹é…ç‡: {overall_metrics['exact_match_rate']:.2%}")
        print(f"   å¹³å‡è¡Œé‡å åº¦: {overall_metrics['average_line_overlap']:.2%}")

        return final_results

    def evaluate_sample(
        self,
        agent_factory: Callable[..., Any],
        sample: Dict[str, Any],
        **agent_kwargs,
    ) -> Dict[str, Any]:
        """è¯„ä¼°å•ä¸ªå®ä¾‹

        Args:
            agent_factory: agent å·¥å‚å‡½æ•°
            sample: SWE-bench å®ä¾‹
            **agent_kwargs: ä¼ é€’ç»™ agent_factory çš„é¢å¤–å‚æ•°

        Returns:
            å•ä¸ªå®ä¾‹çš„è¯„ä¼°ç»“æœ
        """
        instance_id = sample.get("instance_id", "")
        workspace = None

        try:
            # 1. å…‹éš†ä»“åº“å¹¶åˆ‡æ¢åˆ° base_commit
            workspace = self._setup_repo(sample)

            # 2. åˆ›å»º agentï¼ˆworkspace æŒ‡å‘å…‹éš†å‡ºçš„ä»“åº“ï¼‰
            agent = agent_factory(workspace=str(workspace), **agent_kwargs)

            # 3. æ„å»º prompt
            prompt = self._build_prompt(sample)

            # 4. è¿è¡Œ agent
            start_time = time.time()
            agent.run(prompt)
            execution_time = time.time() - start_time

            # 5. æ”¶é›† agent äº§ç”Ÿçš„ patch
            predicted_patch = self._collect_patch(workspace)

            # 6. è®¡ç®—è¡¥ä¸æŒ‡æ ‡
            gold_patch = sample.get("patch", "")
            patch_metrics = self.metrics.calculate_patch_metrics(
                predicted_patch, gold_patch
            )

            # 7. å¯é€‰ï¼šè¿è¡Œæµ‹è¯•
            tests_passed = False
            test_output = ""
            if self.run_tests and sample.get("FAIL_TO_PASS"):
                tests_passed, test_output = self._run_tests(
                    workspace, sample["FAIL_TO_PASS"]
                )

            score = 1.0 if patch_metrics["exact_match"] or tests_passed else 0.0

            return {
                "instance_id": instance_id,
                "repo": sample.get("repo", ""),
                "exact_match": patch_metrics["exact_match"],
                "tests_passed": tests_passed,
                "patch_metrics": patch_metrics,
                "predicted_patch": predicted_patch,
                "score": score,
                "execution_time": execution_time,
                "test_output": test_output,
            }

        except Exception as e:
            return {
                "instance_id": instance_id,
                "repo": sample.get("repo", ""),
                "exact_match": False,
                "tests_passed": False,
                "patch_metrics": {},
                "predicted_patch": "",
                "score": 0.0,
                "error": str(e),
            }
        finally:
            if workspace:
                self._cleanup_workspace(workspace)

    # ------------------------------------------------------------------
    # Internal helpers
    # ------------------------------------------------------------------

    def _setup_repo(self, sample: Dict[str, Any]) -> Path:
        """å…‹éš†ä»“åº“å¹¶åˆ‡æ¢åˆ° base_commit

        Args:
            sample: SWE-bench å®ä¾‹

        Returns:
            å…‹éš†å‡ºçš„ä»“åº“è·¯å¾„
        """
        repo = sample["repo"]
        base_commit = sample["base_commit"]

        workspace = Path(
            tempfile.mkdtemp(
                prefix=f"swe_{sample.get('instance_id', 'unknown')}_",
                dir=self.workspace_base,
            )
        )

        repo_url = f"https://github.com/{repo}.git"

        # æµ…å…‹éš† + checkout ç›®æ ‡ commit
        subprocess.run(
            ["git", "clone", "--no-checkout", repo_url, str(workspace)],
            check=True,
            capture_output=True,
            timeout=300,
        )
        subprocess.run(
            ["git", "checkout", base_commit],
            check=True,
            capture_output=True,
            cwd=str(workspace),
            timeout=60,
        )

        return workspace

    def _collect_patch(self, workspace: Path) -> str:
        """æ”¶é›† agent åœ¨å·¥ä½œç›®å½•ä¸­çš„æ‰€æœ‰æ›´æ”¹

        Args:
            workspace: ä»“åº“å·¥ä½œç›®å½•

        Returns:
            unified diff å­—ç¬¦ä¸²
        """
        result = subprocess.run(
            ["git", "diff", "HEAD"],
            capture_output=True,
            text=True,
            cwd=str(workspace),
            timeout=30,
        )
        return result.stdout

    def _build_prompt(self, sample: Dict[str, Any]) -> str:
        """æ„å»ºå‘é€ç»™ agent çš„ prompt

        Args:
            sample: SWE-bench å®ä¾‹

        Returns:
            prompt å­—ç¬¦ä¸²
        """
        problem = sample.get("problem_statement", "")
        hints = sample.get("hints_text", "")
        repo = sample.get("repo", "")
        instance_id = sample.get("instance_id", "")

        prompt = (
            f"You are working on the repository: {repo}\n"
            f"Instance ID: {instance_id}\n\n"
            f"## GitHub Issue\n\n{problem}\n"
        )

        if hints:
            prompt += f"\n## Hints\n\n{hints}\n"

        prompt += (
            "\n## Instructions\n\n"
            "Please investigate this issue in the codebase and produce a fix. "
            "Explore the relevant source files, understand the root cause, and "
            "make the necessary code changes to resolve the issue. "
            "Do NOT run tests or create new test files â€” only modify source code."
        )

        return prompt

    def _run_tests(
        self, workspace: Path, fail_to_pass: List[str]
    ) -> tuple:
        """è¿è¡Œ FAIL_TO_PASS æµ‹è¯•åˆ—è¡¨

        Args:
            workspace: ä»“åº“å·¥ä½œç›®å½•
            fail_to_pass: éœ€è¦ä» FAIL å˜ä¸º PASS çš„æµ‹è¯•åˆ—è¡¨

        Returns:
            (all_passed, test_output)
        """
        if not fail_to_pass:
            return False, ""

        try:
            result = subprocess.run(
                ["python", "-m", "pytest"] + fail_to_pass + ["-x", "--tb=short"],
                capture_output=True,
                text=True,
                cwd=str(workspace),
                timeout=self.timeout_per_instance,
            )
            all_passed = result.returncode == 0
            return all_passed, result.stdout + result.stderr
        except (subprocess.TimeoutExpired, FileNotFoundError) as e:
            return False, str(e)

    def _cleanup_workspace(self, workspace: Path) -> None:
        """æ¸…ç†ä¸´æ—¶å·¥ä½œç›®å½•"""
        try:
            shutil.rmtree(str(workspace), ignore_errors=True)
        except Exception:
            pass

    def _create_empty_results(self) -> Dict[str, Any]:
        """åˆ›å»ºç©ºçš„è¯„ä¼°ç»“æœ"""
        return {
            "benchmark": "SWE-bench",
            "total_samples": 0,
            "resolved_rate": 0.0,
            "exact_match_rate": 0.0,
            "average_line_overlap": 0.0,
            "average_execution_time": 0.0,
            "repo_metrics": {},
            "detailed_results": [],
        }

    def export_to_swe_format(
        self,
        results: Dict[str, Any],
        output_path: Union[str, Path],
    ) -> None:
        """å¯¼å‡ºä¸º SWE-bench å®˜æ–¹æäº¤æ ¼å¼

        JSONL æ ¼å¼ï¼Œæ¯è¡ŒåŒ…å« instance_id å’Œ model_patchã€‚

        Args:
            results: evaluate() è¿”å›çš„ç»“æœå­—å…¸
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        output_path = Path(output_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)

        detailed = results.get("detailed_results", [])

        with open(output_path, "w", encoding="utf-8") as f:
            for r in detailed:
                entry = {
                    "instance_id": r.get("instance_id", ""),
                    "model_patch": r.get("predicted_patch", ""),
                }
                f.write(json.dumps(entry, ensure_ascii=False) + "\n")

        print(f"âœ… SWE-bench æ ¼å¼ç»“æœå·²å¯¼å‡º")
        print(f"   è¾“å‡ºæ–‡ä»¶: {output_path}")
        print(f"   æ ·æœ¬æ•°: {len(detailed)}")
